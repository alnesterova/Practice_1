---
title: "Упражнение 1"
author: "Нестерова А.И."
date: "03 03 2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Математическое моделирование

### Оценка точности модели с непрерывной зависимой переменной

## Упражнение 1


1.	Завести аккаунт на github.com.    

2.	**Задача 1.** На данных своего варианта повторить три графика из первой практики, выбрав число степеней свободы как компромисс между точностью (оценкой ошибки на тестовой выборке) и простотой модели (числом степеней свободы). Все рисунки сохранить в графические файлы в формате png. 

3.	**Задача 2.** Решить задачу 1, изменив характеристики данных (см. свой вариант). Почему при таком изменении данных MSE меняется именно так? Все рисунки сохранить в графические файлы в формате png.   

4.	Выполненные задачи 1-2 разместить в одном отчёте в репозитории на github.com, выслать ссылку на него на почту преподавателя. В репозитории должны лежать:

- файл README.md с кратким описанием содержимого репозитория;    

- скрипт генерации отчёта: файл .Rmd в кодировке UTF-8;   

- графики, сгенерированные в задании.   


### Задача 1

### Вариант - 13

* $X \sim U(5, 105)$   

* $Y = f(X) + \epsilon$, где $f(X) = 25 +0.02X - 0.003(X-45)^2 - 0,00006(X-54)^3$; $\epsilon \sim N(0, 1)$.

```{r generate-data}
#  Генерируем данные ###########################################################
library('knitr')
my.seed <- 1486372882    # ядро
n.all <- 60              # наблюдений всего
train.percent <- 0.85    # доля обучающей выборки
res.sd <- 1              # стандартное отклонение случайного шума
x.min <- 5               # нижняя граница изменения X
x.max <- 105             # и верхняя граница изменения X

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

# случайный шум
set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# истинная функция взаимосвязи
y.func <- function(x) {25 + 2e-02*x - 3e-03*(x-45)^2 - 6e-05*(x-54)^3}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]

```

Изобразим исходные данные на графике.

```{r plot-1, fig.height = 5, fig.width = 5}
#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и истинная функция связи', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, lwd = 2, lty = 2)

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)
```

В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40 (количество узлов равно 2/3 наблюдений). Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках.    

```{r models}
#  Строим модели с df от 2 до 40 ########################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40

tbl <- data.frame(df = 2:max.df)   # таблица для записи ошибок
tbl$MSE.train <- 0                 # столбец: ошибки на обучающей выборке
tbl$MSE.test <- 0                  # столбец: ошибки на тестовой выборке

# цикл по степеням свободы
for (i in 2:max.df) {
  # строим модель
  mod <- smooth.spline(x = x.train, y = y.train, df = i)
  
  # модельные значения для расчёта ошибок
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  
  # записываем ошибки в таблицу
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)
```

Изобразим на графике поведение ошибок при различном количестве степеней свободы.  
```{r plot-2, fig.height = 5, fig.width = 5}

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Изменение MSE с ростом числа степеней свободы', side = 3)

points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test <- min(tbl$MSE.test)
df.min.MSE.test <- tbl[tbl$MSE.test == min.MSE.test, 'df']

# компромисс между точностью и простотой модели по графику
df.my.MSE.test <- 6
my.MSE.test <- tbl[tbl$df == df.my.MSE.test, 'MSE.test']

# ставим точку на графике
abline(v = df.my.MSE.test, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test, 
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

```

На этом графике:   

* При движении слева направо MSE на обучающей выборке (серая кривая) сокращается, потому что с ростом числа степеней свободы расчёт число узлов, по которым строится сплайн. При этом модельная кривая подгоняется по всё возрастающему количеству точек и становится всё более гибкой. В результате индивидуальные расстояния от фактических наблюдений за $Y$ до их модельных оценок сокращаются, что приводит к сокращению MSE.   

* При движении слева направо MSE на тестовой выборке (красная кривая) сначала резко сокращается, затем растёт. Нам известна истинная форма связи $Y$ с $X$, она описывается кубической функцией. Число степеней свободы такой модели равно числу оцениваемых параметров, т.е. 4 (коэффициенты перед $X$, $X^2$, $X^3$ и константа). Поэтому резкое падение ошибки на тестовой выборке при небольшом числе степеней свободы связано с тем, что модель приближается по гибкости к истинной функции связи. Затем MSE на тестовой выборке довольно долго остаётся стабильной, а затем начинает расти. Этот рост объясняется эффектом переобучения модели: она всё лучше описывает обучающую выборку, и при этом постепенно становится неприменимой ни к одному другому набору наблюдений.   

Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы `r df.min.MSE.test` и равно `r round(min.MSE.test, 2)`. Визуально по графику мы можем установить, что первое значение $MSE_{ТЕСТ}$, близкое к стабильно низким, соответствует df = `r df.my.MSE.test`. Ошибка здесь равна `r round(my.MSE.test)`, что ненамного отличается от минимума. Именно df = `r df.my.MSE.test` было выбрано в качестве компромисса между точностью (минимальной MSE на тестовой выборке) и простотой модели (чем меньше степеней свободы, тем модель проще).    

График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.   

```{r plot-3, fig.height = 5, fig.width = 5}
#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и лучшая модель', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue')

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

### Задача 2

### Вариант - 13

Проверим, как будет меняться оценка $MSE$ лучшей модели при изменении количества наблюдений $n$

* $n = 600$ 


```{r specifications 1}
# генерируем данные: n = 600
n.all <- 600   

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

# случайный шум
set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# истинная функция взаимосвязи
y.func <- function(x) {25 + 2e-02*x - 3e-03*(x-45)^2 - 6e-05*(x-54)^3}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]

```

Изобразим исходные данные на графике.

```{r plot-1.1, fig.height = 5, fig.width = 5}
#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и истинная функция связи n = 600', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, lwd = 2, lty = 2)

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)
```

В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40. Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках.    

```{r models 1}
#  Строим модели с df от 2 до 40 ########################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40

tbl <- data.frame(df = 2:max.df)   # таблица для записи ошибок
tbl$MSE.train <- 0                 # столбец: ошибки на обучающей выборке
tbl$MSE.test <- 0                  # столбец: ошибки на тестовой выборке

# цикл по степеням свободы
for (i in 2:max.df) {
  # строим модель
  mod <- smooth.spline(x = x.train, y = y.train, df = i)
  
  # модельные значения для расчёта ошибок
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  
  # записываем ошибки в таблицу
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)
```

Изобразим на графике поведение ошибок при различном количестве степеней свободы.  
```{r plot-2.1, fig.height = 5, fig.width = 5}

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Изменение MSE с ростом числа степеней свободы n = 600', side = 3)

points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test1 <- min(tbl$MSE.test)
df.min.MSE.test1 <- tbl[tbl$MSE.test == min.MSE.test1, 'df']

# компромисс между точностью и простотой модели по графику
df.my.MSE.test1 <- 6
my.MSE.test1 <- tbl[tbl$df == df.my.MSE.test1, 'MSE.test']

# ставим точку на графике
abline(v = df.my.MSE.test1, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test1, y = my.MSE.test1, 
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test1, col = 'blue', cex = 1.2)

```

На этом графике:   

Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы `r df.min.MSE.test1` и равно `r round(min.MSE.test1, 2)`. Визуально по графику мы можем установить, что первое значение $MSE_{ТЕСТ}$, близкое к стабильно низким, соответствует df = `r df.my.MSE.test1`. Ошибка здесь равна `r round(my.MSE.test1)`, что ненамного отличается от минимума. Именно df = `r df.my.MSE.test1` было выбрано в качестве компромисса между точностью (минимальной MSE на тестовой выборке) и простотой модели (чем меньше степеней свободы, тем модель проще).    

График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.   

```{r plot-3.1, fig.height = 5, fig.width = 5}
#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test1)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и лучшая модель (n = 600)', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue')

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

* $n = 550$

```{r specifications 2}
# генерируем данные: n = 550
n.all <- 550

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

# случайный шум
set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# истинная функция взаимосвязи
y.func <- function(x) {25 + 2e-02*x - 3e-03*(x-45)^2 - 6e-05*(x-54)^3}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]

```

Изобразим исходные данные на графике.

```{r plot-1.2,fig.height = 5, fig.width = 5}
#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и истинная функция связи n = 550', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, lwd = 2, lty = 2)

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)
```

В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40. Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках.    

```{r models 2}
#  Строим модели с df от 2 до 40 ########################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40

tbl <- data.frame(df = 2:max.df)   # таблица для записи ошибок
tbl$MSE.train <- 0                 # столбец: ошибки на обучающей выборке
tbl$MSE.test <- 0                  # столбец: ошибки на тестовой выборке

# цикл по степеням свободы
for (i in 2:max.df) {
  # строим модель
  mod <- smooth.spline(x = x.train, y = y.train, df = i)
  
  # модельные значения для расчёта ошибок
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  
  # записываем ошибки в таблицу
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)
```

Изобразим на графике поведение ошибок при различном количестве степеней свободы.  
```{r plot-2.2, fig.height = 5, fig.width = 5}

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Изменение MSE с ростом числа степеней свободы n = 550', side = 3)

points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test2 <- min(tbl$MSE.test)
df.min.MSE.test2 <- tbl[tbl$MSE.test == min.MSE.test2, 'df']

# компромисс между точностью и простотой модели по графику
df.my.MSE.test2 <- 5
my.MSE.test2 <- tbl[tbl$df == df.my.MSE.test2, 'MSE.test']

# ставим точку на графике
abline(v = df.my.MSE.test2, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test1, y = my.MSE.test2, 
       pch = 15, col = 'blue')
mtext(df.my.MSE.test2, 
      side = 1, line = -1, at = df.my.MSE.test2, col = 'blue', cex = 1.2)

```

На этом графике:   

Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы `r df.min.MSE.test2` и равно `r round(min.MSE.test2, 2)`. Визуально по графику мы можем установить, что первое значение $MSE_{ТЕСТ}$, близкое к стабильно низким, соответствует df = `r df.my.MSE.test2`. Ошибка здесь равна `r round(my.MSE.test2)`, что ненамного отличается от минимума. Именно df = `r df.my.MSE.test2` было выбрано в качестве компромисса между точностью (минимальной MSE на тестовой выборке) и простотой модели (чем меньше степеней свободы, тем модель проще).    

График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.   

```{r plot-3.2, fig.height = 5, fig.width = 5}
#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test2)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и лучшая модель (n = 550)', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue')

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```

* $n = 500$

```{r specifications 3}
# генерируем данные: n = 500
n.all <- 500

# фактические значения x
set.seed(my.seed)
x <- runif(x.min, x.max, n = n.all)

# случайный шум
set.seed(my.seed)
res <- rnorm(mean = 0, sd = res.sd, n = n.all)

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# истинная функция взаимосвязи
y.func <- function(x) {25 + 2e-02*x - 3e-03*(x-45)^2 - 6e-05*(x-54)^3}

# для графика истинной взаимосвязи
x.line <- seq(x.min, x.max, length = n.all)
y.line <- y.func(x.line)

# фактические значения y (с шумом)
y <- y.func(x) + res

# отбираем наблюдения в обучающую выборку
set.seed(my.seed)
inTrain <- sample(seq_along(x), size = train.percent*n.all)

# Создаём векторы с данными для построения графиков ############################

# наблюдения на обучающей выборке
x.train <- x[inTrain]
y.train <- y[inTrain]

# наблюдения на тестовой выборке
x.test <- x[-inTrain]
y.test <- y[-inTrain]

```

Изобразим исходные данные на графике.

```{r plot-1.3, fig.height = 5, fig.width = 5}
#  График 1: Исходные данные на график #########################################

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и истинная функция связи n = 500', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, lwd = 2, lty = 2)

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)'),
       pch = c(16, 16, NA), 
       col = c(grey(0.2), 'red', 'black'),  
       lty = c(0, 0, 2), lwd = c(1, 1, 2), cex = 1.2)
```

В качестве модели используем сплайны со степенями свободы от 2 (прямая) до 40. Строим модели с различным количеством степеней свободы и в каждом случае считаем среднеквадратическую ошибку модели на обучающей и тестовой выборках.    

```{r models 3}
#  Строим модели с df от 2 до 40 ########################################

# максимальное число степеней свободы для модели сплайна
max.df <- 40

tbl <- data.frame(df = 2:max.df)   # таблица для записи ошибок
tbl$MSE.train <- 0                 # столбец: ошибки на обучающей выборке
tbl$MSE.test <- 0                  # столбец: ошибки на тестовой выборке

# цикл по степеням свободы
for (i in 2:max.df) {
  # строим модель
  mod <- smooth.spline(x = x.train, y = y.train, df = i)
  
  # модельные значения для расчёта ошибок
  y.model.train <- predict(mod, data.frame(x = x.train))$y[, 1]
  y.model.test <- predict(mod, data.frame(x = x.test))$y[, 1]
  
  # считаем средний квадрат ошибки на обучающей и тестовой выборке
  MSE <- c(sum((y.train - y.model.train)^2) / length(x.train),
           sum((y.test - y.model.test)^2) / length(x.test))
  
  # записываем ошибки в таблицу
  tbl[tbl$df == i, c('MSE.train', 'MSE.test')] <- MSE
}

# первые строки таблицы
head(tbl)
```

Изобразим на графике поведение ошибок при различном количестве степеней свободы.  
```{r plot-2.3, fig.height = 5, fig.width = 5}

#  График 2: Зависимость MSE от гибкости модели ################################

plot(x = tbl$df, y = tbl$MSE.test, 
     type = 'l', col = 'red', lwd = 2,
     xlab = 'Степени свободы сплайна', ylab = 'MSE',
     ylim = c(min(tbl$MSE.train, tbl$MSE.test), 
              max(tbl$MSE.train, tbl$MSE.test)),
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Изменение MSE с ростом числа степеней свободы n = 500', side = 3)

points(x = tbl$df, y = tbl$MSE.test,
       pch = 21, col = 'red', bg = 'red')
lines(x = tbl$df, y = tbl$MSE.train, col = grey(0.3), lwd = 2)
# неустранимая ошибка
abline(h = res.sd, lty = 2, col = grey(0.4), lwd = 2)

# легенда
legend('topright', legend = c('обучающая', 'тестовая'),
       pch = c(NA, 16), 
       col = c(grey(0.2), 'red'),  
       lty = c(1, 1), lwd = c(2, 2), cex = 1.2)

# степени свободы у наименьшей ошибки на тестовой выборке
min.MSE.test3 <- min(tbl$MSE.test)
df.min.MSE.test3 <- tbl[tbl$MSE.test == min.MSE.test3, 'df']

# компромисс между точностью и простотой модели по графику
df.my.MSE.test3 <- 6
my.MSE.test3 <- tbl[tbl$df == df.my.MSE.test3, 'MSE.test']

# ставим точку на графике
abline(v = df.my.MSE.test3, 
       lty = 2, lwd = 2)
points(x = df.my.MSE.test, y = my.MSE.test3, 
       pch = 15, col = 'blue')
mtext(df.my.MSE.test, 
      side = 1, line = -1, at = df.my.MSE.test, col = 'blue', cex = 1.2)

```

На этом графике:   

Наименьшее значение MSE на тестовой выборке соответствует числу степеней свободы `r df.min.MSE.test3` и равно `r round(min.MSE.test3, 2)`. Визуально по графику мы можем установить, что первое значение $MSE_{ТЕСТ}$, близкое к стабильно низким, соответствует df = `r df.my.MSE.test3`. Ошибка здесь равна `r round(my.MSE.test3)`, что ненамного отличается от минимума. Именно df = `r df.my.MSE.test3` было выбрано в качестве компромисса между точностью (минимальной MSE на тестовой выборке) и простотой модели (чем меньше степеней свободы, тем модель проще).    

График с моделью, выбранной в качестве лучшей, показан на рисунке ниже.   

```{r plot-3.3, fig.height = 5, fig.width = 5}
#  График 3: Лучшая модель (компромисс между гибкостью и точностью) ############

mod.MSE.test <- smooth.spline(x = x.train, y = y.train, df = df.my.MSE.test3)

# для гладких графиков модели
x.model.plot <- seq(x.min, x.max, length = 250)
y.model.plot <- predict(mod.MSE.test, data.frame(x = x.model.plot))$y[, 1]

# убираем широкие поля рисунка
par(mar = c(4, 4, 1, 1))

# наименьшие/наибольшие значения по осям
x.lim <- c(x.min, x.max)
y.lim <- c(min(y), max(y))

# наблюдения с шумом (обучающая выборка)
plot(x.train, y.train, 
     col = grey(0.2), bg = grey(0.2), pch = 21,
     xlab = 'X', ylab = 'Y', 
     xlim = x.lim, ylim = y.lim, 
     cex = 1.2, cex.lab = 1.2, cex.axis = 1.2)

# заголовок
mtext('Исходные данные и лучшая модель (n = 500)', side = 3)

# наблюдения тестовой выборки
points(x.test, y.test, 
       col = 'red', bg = 'red', pch = 21)

# истинная функция
lines(x.line, y.line, 
      lwd = 2, lty = 2)

# модель
lines(x.model.plot, y.model.plot, 
      lwd = 2, col = 'blue')

# легенда
legend('bottomleft', legend = c('обучение', 'тест', 'f(X)', 'модель'),
       pch = c(16, 16, NA, NA), 
       col = c(grey(0.2), 'red', 'black', 'blue'),  
       lty = c(0, 0, 2, 1), lwd = c(1, 1, 2, 2), cex = 1.2)
```
```{r}
MSE <- c(my.MSE.test1, my.MSE.test2, my.MSE.test3)
MSE
```

Из графика зависимости показателей $MSE$ от количества наблюдений имееем, что наибольшее значение ошибки будет при значении $n = 550$, наименьшеее - $n = 600$.